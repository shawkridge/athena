"""Prospective memory handler methods for MCP server."""

import json
import logging
from typing import Any, Dict, List, Optional
from datetime import datetime
from mcp.types import TextContent

from .structured_result import StructuredResult, ResultStatus, PaginationMetadata, create_paginated_result, paginate_results
from ..core.models import MemoryType
from ..prospective.models import ProspectiveTask, TaskStatus, TaskPriority, TaskPhase

logger = logging.getLogger(__name__)


class ProspectiveHandlersMixin:
    """Prospective memory handler methods (24 methods, ~1290 lines).

    Extracted from monolithic handlers.py as part of Phase 4 refactoring.
    Provides prospective memory operations: task management, goal tracking,
    milestone management, and task planning.

    Methods:
    - _handle_create_task: Create new task
    - _handle_list_tasks: List tasks with filtering
    - _handle_update_task_status: Update task status
    - _handle_create_task_with_planning: Create task with plan
    - _handle_start_task: Start task execution
    - _handle_verify_task: Verify task completion
    - _handle_create_task_with_milestones: Create task with milestones
    - _handle_update_milestone_progress: Update milestone progress
    - _handle_get_active_goals: Get active goals
    - _handle_set_goal: Create/update goal
    - _handle_activate_goal: Activate goal
    - _handle_complete_goal: Complete goal
    - _handle_get_goal_priority_ranking: Rank goals by priority
    - _handle_recommend_next_goal: Recommend next goal
    - _handle_check_goal_conflicts: Check goal conflicts
    - _handle_resolve_goal_conflicts: Resolve goal conflicts
    - _handle_get_task_health: Get task health metrics
    - _handle_get_project_status: Get project status
    - _handle_create_task_from_template: Create from template
    - _handle_generate_task_plan: Generate task plan
    - _handle_calculate_task_cost: Calculate task cost
    - _handle_predict_task_duration: Predict duration
    - _handle_estimate_task_resources_detailed: Estimate resources
    - _handle_analyze_task_analytics_detailed: Analyze analytics
    """

    async def _handle_create_task(self, args: dict) -> list[TextContent]:
        """Handle create_task tool call with auto-entity creation.

        PHASE 5 AUTO-INTEGRATION: Automatically creates Knowledge Graph entity
        for the task to enable cross-layer linking and graph queries.
        """
        project = self.project_manager.get_or_create_project()

        from ..prospective.models import TaskTrigger, TriggerType

        # Auto-generate active_form if not provided (convert imperative to present continuous)
        content = args.get("content", "")
        active_form = args.get("active_form")
        if not active_form:
            # Convert content to present continuous form
            # e.g. "Implement feature" -> "Implementing feature"
            words = content.split()
            if words and words[0].endswith(('e',)):  # Handle words ending in 'e'
                active_form = words[0][:-1] + 'ing' + ' ' + ' '.join(words[1:])
            elif words:
                active_form = words[0] + 'ing' + ' ' + ' '.join(words[1:])
            else:
                active_form = "Working"

        task = ProspectiveTask(
            project_id=project.id,
            content=content,
            active_form=active_form,
            priority=TaskPriority(args.get("priority", "medium")),
            status=TaskStatus.PENDING,
            created_at=datetime.now(),
        )

        task_id = self.prospective_store.create_task(task)

        # Add triggers if provided
        if "triggers" in args:
            for trigger_data in args["triggers"]:
                trigger = TaskTrigger(
                    task_id=task_id,
                    trigger_type=TriggerType(trigger_data["type"]),
                    trigger_value=trigger_data["value"],
                    trigger_condition={},
                )
                self.prospective_store.add_trigger(trigger)

        # ===== PHASE 5 AUTO-ENTITY CREATION =====
        # Auto-create Knowledge Graph entity for the task
        entity_created = False
        try:
            task_entity = Entity(
                name=f"Task: {task.content[:100]}",  # Truncate to 100 chars
                entity_type=EntityType.TASK,
                project_id=project.id,
                created_at=datetime.now(),
                metadata={
                    "task_id": task_id,
                    "priority": str(task.priority),
                    "status": str(task.status),
                }
            )
            entity_id = self.graph_store.create_entity(task_entity)

            # Create CONTAINS relation from project to task
            try:
                from ..graph.models import Relation, RelationType
                # Find or create project entity
                project_entities = self.graph_store.search_entities(f"Project: {project.name}")
                if project_entities:
                    project_entity_id = project_entities[0].id
                    relation = Relation(
                        from_entity_id=project_entity_id,
                        to_entity_id=entity_id,
                        relation_type=RelationType.CONTAINS,
                        created_at=datetime.now(),
                    )
                    self.graph_store.create_relation(relation)
            except Exception as e:
                logger.debug(f"Could not create projectâ†’task relation: {e}")

            entity_created = True
        except Exception as e:
            logger.error(f"Error auto-creating task entity [task_id={task_id}]: {e}", exc_info=True)

        response = f"âœ“ Created task (ID: {task_id})\n"
        response += f"Content: {task.content}\n"
        response += f"Priority: {task.priority}\n"
        if "triggers" in args:
            response += f"Triggers: {len(args['triggers'])} added\n"
        if entity_created:
            response += f"Graph Entity: âœ“ Created (auto-integration)"

        return [TextContent(type="text", text=response)]


    async def _handle_list_tasks(self, args: dict) -> list[TextContent]:
        """Handle list_tasks tool call."""
        try:
            project = await self.project_manager.require_project()

            status_filter = TaskStatus(args["status"]) if "status" in args else None
            priority_filter = TaskPriority(args["priority"]) if "priority" in args else None

            # Pagination parameters (Anthropic pattern)
            limit = min(args.get("limit", 10), 100)
            offset = args.get("offset", 0)

            tasks = self.prospective_store.list_tasks(
                project.id,
                status=status_filter,
                limit=limit,
                offset=offset
            )

            # Get total count for pagination
            total_count = self.prospective_store.count_tasks(
                project.id,
                status=status_filter
            )

            # Format tasks for structured response
            formatted_tasks = []
            for task in tasks:
                formatted_tasks.append({
                    "id": task.id,
                    "content": task.content,
                    "status": task.status.value if hasattr(task.status, 'value') else str(task.status),
                    "priority": task.priority.value if hasattr(task.priority, 'value') else str(task.priority),
                    "due_at": task.due_at.isoformat() if task.due_at else None,
                    "created_at": task.created_at.isoformat() if task.created_at else None,
                })

            # Use standard pagination pattern
            result = paginate_results(
                results=formatted_tasks,
                args=args,
                total_count=total_count,
                operation="list_tasks",
                drill_down_hint="Use /get-task with task_id for full task details including triggers and history"
            )
        except Exception as e:
            result = StructuredResult.error(str(e), metadata={"operation": "list_tasks"})

        return [result.as_optimized_content(schema_name="prospective")]


    async def _handle_update_task_status(self, args: dict) -> list[TextContent]:
        """Handle update_task_status tool call.

        Includes taskâ†’consolidation sync:
        - Records episodic event when task completes
        - Detects repeated task patterns
        - Triggers consolidation if pattern found
        """
        task_id = args["task_id"]
        status_input = args.get("status", "").lower()

        # Map common status aliases
        status_mapping = {
            "in_progress": "active",
            "inprogress": "active",
            "in progress": "active",
            "working": "active",
            "started": "active",
            "done": "completed",
            "finished": "completed",
            "stopped": "cancelled",
            "stuck": "blocked",
        }

        # Use mapped status or original
        mapped_status = status_mapping.get(status_input, status_input)
        new_status = TaskStatus(mapped_status)
        notes = args.get("notes")

        # Get task before updating
        task = self.prospective_store.get_task(task_id)
        if not task:
            return [TextContent(type="text", text=f"âœ— Task {task_id} not found")]

        # Update task status
        self.prospective_store.update_task_status(task_id, new_status, notes)

        response = f"âœ“ Updated task {task_id} status to {new_status}"
        if notes:
            response += f"\nNotes: {notes}"

        # ===== TASKâ†’CONSOLIDATION SYNCHRONIZATION =====
        if new_status == TaskStatus.COMPLETED:
            # Record completion event to episodic layer
            completion_event = EpisodicEvent(
                project_id=task.project_id,
                session_id="task-completion",
                event_type=EventType.SUCCESS,
                content=f"Completed task: {task.content}",
                outcome=EventOutcome.SUCCESS,
                context=EventContext(
                    task=task.content,
                    phase="task-completion",
                ),
            )
            self.episodic_store.record_event(completion_event)

            # Detect patterns in recently completed tasks
            patterns = self.prospective_store.detect_task_patterns(
                project_id=task.project_id,
                min_occurrences=2,
                time_window_hours=168,  # 1 week
            )

            if patterns:
                response += f"\n\nðŸ“Š Detected {len(patterns)} task pattern(s):"
                for pattern in patterns:
                    response += f"\n  â€¢ {pattern['content']} ({pattern['count']} occurrences)"
                    response += f"\n    â†’ Suggest: `/workflow create \"{pattern['content']} workflow\"`"

                # Trigger background consolidation for discovered patterns
                # (Would be async in production, but run inline for now)
                try:
                    # Record pattern as discovered for later consolidation
                    for pattern in patterns:
                        # Log as episodic event for consolidation to pick up
                        pattern_event = EpisodicEvent(
                            project_id=task.project_id,
                            session_id="task-pattern-detection",
                            event_type=EventType.DECISION,
                            content=f"Pattern detected: {pattern['content']} ({pattern['count']}x)",
                            outcome=EventOutcome.SUCCESS,
                            context=EventContext(
                                task=f"Pattern: {pattern['content']}",
                                phase="pattern-analysis",
                            ),
                        )
                        self.episodic_store.record_event(pattern_event)

                    response += f"\n\nâœ¨ Patterns recorded for consolidation (run `/consolidate` to extract workflows)"
                except Exception as e:
                    logger.error(f"Error recording pattern events: {e}")
                    response += f"\nâš ï¸ Pattern detection recorded but consolidation trigger failed"

        return [TextContent(type="text", text=response)]


    async def _handle_create_task_with_planning(self, args: dict) -> list[TextContent]:
        """Handle create_task_with_planning tool call.

        Creates a task in PLANNING phase and optionally generates an execution plan.

        Args:
            args: Tool args including:
                - content: Task description
                - active_form: Active form for the task (auto-generated if not provided)
                - priority: Task priority (default: medium)
                - estimated_minutes: Estimated duration (default: 30)
                - auto_plan: Whether to auto-generate plan (default: True)
        """
        project = self.project_manager.get_or_create_project()

        # Create task in PLANNING phase
        from ..prospective.models import TaskPhase

        # Auto-generate active_form if not provided
        content = args.get("content", "")
        active_form = args.get("active_form")
        if not active_form:
            words = content.split()
            if words and words[0].endswith(('e',)):
                active_form = words[0][:-1] + 'ing' + ' ' + ' '.join(words[1:])
            elif words:
                active_form = words[0] + 'ing' + ' ' + ' '.join(words[1:])
            else:
                active_form = "Working"

        task = ProspectiveTask(
            project_id=project.id,
            content=content,
            active_form=active_form,
            priority=TaskPriority(args.get("priority", "medium")),
            status=TaskStatus.PENDING,
            phase=TaskPhase.PLANNING,
            created_at=datetime.now(),
        )

        task_id = self.prospective_store.create_task(task)

        response = f"âœ“ Created task (ID: {task_id}) in PLANNING phase\n"
        response += f"Content: {task.content}\n"
        response += f"Priority: {task.priority}\n"
        response += f"Phase: PLANNING\n"

        # Auto-generate plan if requested
        auto_plan = args.get("auto_plan", True)
        estimated_minutes = args.get("estimated_minutes", 30)

        if auto_plan:
            try:
                # Create basic plan steps from task content
                # Note: For full hierarchical decomposition, use /decompose-hierarchically in memory MCP
                keywords = task.content.lower()

                # Smart plan generation based on task content
                if any(word in keywords for word in ["implement", "code", "write", "develop", "create"]):
                    steps = [
                        "Design architecture and approach",
                        "Set up project structure",
                        "Implement core functionality",
                        "Add error handling and edge cases",
                        "Test implementation",
                        "Code review and refinement",
                        "Document code and API",
                    ]
                elif any(word in keywords for word in ["debug", "fix", "troubleshoot", "issue"]):
                    steps = [
                        "Identify problem scope",
                        "Reproduce the issue",
                        "Analyze root cause",
                        "Implement fix",
                        "Test fix",
                        "Verify no regressions",
                        "Document solution",
                    ]
                elif any(word in keywords for word in ["refactor", "optimize", "improve", "enhance"]):
                    steps = [
                        "Analyze current implementation",
                        "Identify optimization opportunities",
                        "Plan refactoring approach",
                        "Implement changes",
                        "Test refactored code",
                        "Verify performance improvements",
                        "Update documentation",
                    ]
                elif any(word in keywords for word in ["research", "analyze", "investigate", "explore"]):
                    steps = [
                        "Gather information and sources",
                        "Read and synthesize findings",
                        "Analyze patterns and insights",
                        "Create summary/report",
                        "Review and validate",
                        "Document conclusions",
                    ]
                else:
                    # Default generic steps
                    steps = [
                        "Plan and design approach",
                        "Implement or execute main work",
                        "Test and validate results",
                        "Review and refine",
                        "Complete and document",
                    ]

                # Create plan for the task
                plan_task = self.prospective_store.create_plan_for_task(
                    task_id=task_id,
                    steps=steps,
                    estimated_duration_minutes=estimated_minutes
                )

                if plan_task and plan_task.plan:
                    response += f"\nâœ… Auto-generated plan with {len(steps)} steps:\n"
                    for i, step in enumerate(steps, 1):
                        response += f"  {i}. {step}\n"
                    response += f"\nEstimated duration: {estimated_minutes} minutes"
                    response += f"\n\nðŸ“‹ Next steps:"
                    response += f"\n1. Review plan and validate steps"
                    response += f"\n2. Call: /plan-validate to formally validate the plan"
                    response += f"\n3. Then transition to PLAN_READY phase via update_task_phase()"
                    response += f"\n\nðŸ’¡ Tip: For detailed hierarchical decomposition, use the memory MCP tool:"
                    response += f"\n   decompose_hierarchically(task_description=..., complexity_level=5)"
                else:
                    response += f"\nâš ï¸ Could not generate plan"

            except Exception as e:
                logger.error(f"Error auto-planning task {task_id}: {e}")
                response += f"\nâš ï¸ Plan auto-generation failed: {str(e)}"
                response += f"\nYou can still create a manual plan for this task"
        else:
            response += "\nPlan auto-generation disabled (auto_plan=False)"
            response += "\nNext: Create a manual plan or enable auto_plan=True"

        return [TextContent(type="text", text=response)]


    async def _handle_start_task(self, args: dict) -> list[TextContent]:
        """Handle start_task tool call.

        Transitions task from PLAN_READY to EXECUTING phase.
        Also updates task status to 'in_progress'.

        Preconditions:
        - Task must exist
        - Task must be in PLAN_READY phase
        """
        task_id = args["task_id"]

        # Get task
        task = self.prospective_store.get_task(task_id)
        if not task:
            return [TextContent(type="text", text=f"âœ— Task {task_id} not found")]

        # Verify task is in PLAN_READY phase
        if task.phase != "plan_ready" and task.phase != TaskPhase.PLAN_READY.value:
            return [TextContent(
                type="text",
                text=f"âœ— Task {task_id} is in {task.phase} phase, not PLAN_READY\n"
                     f"Tasks must be in PLAN_READY phase to start execution"
            )]

        # Transition to EXECUTING phase
        try:
            from ..integration.phase_tracking import PhaseTracker

            # Update phase (TaskPhase already imported at top)
            updated_task = self.prospective_store.update_task_phase(
                task_id,
                TaskPhase.EXECUTING
            )

            # Update status to in_progress
            self.prospective_store.update_task_status(
                task_id,
                TaskStatus.ACTIVE
            )

            # Record phase transition if tracker available
            try:
                phase_tracker = PhaseTracker(self.store.db)
                # Note: In production, would use await, but keeping synchronous for now
                phase_tracker.episodic_store.record_event(
                    EpisodicEvent(
                        project_id=task.project_id,
                        session_id="task-execution",
                        event_type=EventType.ACTION,
                        content=f"Task started: {task.content}",
                        outcome=EventOutcome.ONGOING,
                        context=EventContext(
                            task=f"Task {task_id}: {task.content}",
                            phase="execution-started",
                        ),
                    )
                )
            except Exception as e:
                logger.warning(f"Failed to record phase transition event: {e}")

            response = f"âœ“ Task {task_id} started (PLAN_READY â†’ EXECUTING)\n"
            response += f"Content: {task.content}\n"
            response += f"Status: in_progress\n"
            if updated_task and updated_task.plan:
                response += f"\nðŸ“‹ Execution plan ({len(updated_task.plan.steps)} steps):\n"
                for i, step in enumerate(updated_task.plan.steps, 1):
                    response += f"  {i}. {step}\n"
            response += f"\nðŸ’¡ Next: Execute plan steps, then call verify_task() to check completion"

            return [TextContent(type="text", text=response)]

        except Exception as e:
            logger.error(f"Error starting task {task_id}: {e}")
            return [TextContent(
                type="text",
                text=f"âœ— Failed to start task: {str(e)}"
            )]


    async def _handle_verify_task(self, args: dict) -> list[TextContent]:
        """Handle verify_task tool call.

        Transitions task to VERIFYING phase to check if work is complete.
        Optionally completes the task and records metrics.

        Args:
            task_id: Task ID to verify
            completion_notes: Optional notes on completion
            mark_complete: Whether to mark task as COMPLETED
        """
        task_id = args["task_id"]
        completion_notes = args.get("completion_notes")
        mark_complete = args.get("mark_complete", False)

        # Get task
        task = self.prospective_store.get_task(task_id)
        if not task:
            return [TextContent(type="text", text=f"âœ— Task {task_id} not found")]

        response = f"âœ“ Task {task_id} verification started\n"
        response += f"Content: {task.content}\n"

        # Transition to VERIFYING phase
        try:
            from ..prospective.models import TaskPhase

            verifying_task = self.prospective_store.update_task_phase(
                task_id,
                TaskPhase.VERIFYING
            )

            response += f"Phase: VERIFYING\n"

            if completion_notes:
                response += f"Notes: {completion_notes}\n"

            # If mark_complete is requested, transition to COMPLETED
            if mark_complete:
                completed_task = self.prospective_store.complete_phase(
                    task_id,
                    TaskPhase.COMPLETED
                )

                # Update status to completed
                self.prospective_store.update_task_status(
                    task_id,
                    TaskStatus.COMPLETED,
                    reason="Task completed and verified"
                )

                if completed_task:
                    response += f"\nâœ… Task marked COMPLETED\n"
                    if completed_task.actual_duration_minutes:
                        hours = completed_task.actual_duration_minutes / 60
                        response += f"Total duration: {completed_task.actual_duration_minutes:.1f} minutes ({hours:.1f} hours)\n"

                    # Show phase breakdown
                    if completed_task.phase_metrics:
                        response += f"\nðŸ“Š Phase breakdown:\n"
                        for metric in completed_task.phase_metrics:
                            phase_name = metric.phase.value if hasattr(metric.phase, "value") else metric.phase
                            if metric.duration_minutes:
                                response += f"  â€¢ {phase_name}: {metric.duration_minutes:.1f} min\n"
            else:
                response += f"\nðŸ“‹ Verification phase started\n"
                response += f"Review the work and then call verify_task() again with mark_complete=true to complete"

            # Record event
            try:
                event_type = EventType.DECISION if mark_complete else EventType.ACTION
                outcome = EventOutcome.SUCCESS if mark_complete else EventOutcome.ONGOING

                from ..integration.phase_tracking import PhaseTracker
                phase_tracker = PhaseTracker(self.store.db)
                phase_tracker.episodic_store.record_event(
                    EpisodicEvent(
                        project_id=task.project_id,
                        session_id="task-verification",
                        event_type=event_type,
                        content=f"Task verification: {task.content}",
                        outcome=outcome,
                        context=EventContext(
                            task=f"Task {task_id}: {task.content}",
                            phase="verification" if not mark_complete else "completion",
                        ),
                    )
                )
            except Exception as e:
                logger.warning(f"Failed to record verification event: {e}")

            return [TextContent(type="text", text=response)]

        except Exception as e:
            logger.error(f"Error verifying task {task_id}: {e}")
            return [TextContent(
                type="text",
                text=f"âœ— Failed to verify task: {str(e)}"
            )]


    async def _handle_create_task_with_milestones(self, args: dict) -> list[TextContent]:
        """Handle create_task_with_milestones tool call.

        Creates a task with explicit milestones for granular progress tracking.
        Validates milestone percentages sum to ~100%.
        """
        project = self.project_manager.get_or_create_project()

        from ..prospective.models import TaskPhase
        from ..prospective.milestones import Milestone, CheckpointType

        # Auto-generate active_form if not provided
        content = args.get("content", "")
        active_form = args.get("active_form")
        if not active_form:
            words = content.split()
            if words and words[0].endswith(('e',)):
                active_form = words[0][:-1] + 'ing' + ' ' + ' '.join(words[1:])
            elif words:
                active_form = words[0] + 'ing' + ' ' + ' '.join(words[1:])
            else:
                active_form = "Working"

        # Create task
        task = ProspectiveTask(
            project_id=project.id,
            content=content,
            active_form=active_form,
            priority=TaskPriority(args.get("priority", "medium")),
            status=TaskStatus.PENDING,
            phase=TaskPhase.PLANNING,
            created_at=datetime.now(),
        )

        task_id = self.prospective_store.create_task(task)

        response = f"âœ“ Created task (ID: {task_id}) with milestones\n"
        response += f"Content: {task.content}\n"
        response += f"Priority: {task.priority}\n\n"

        # Parse and validate milestones
        try:
            milestones_data = args.get("milestones", [])
            estimated_total = args.get("estimated_minutes", 120)

            if not milestones_data:
                return [TextContent(
                    type="text",
                    text=f"âœ— Task created but no milestones provided"
                )]

            # Validate milestone percentages
            total_percentage = sum(m.get("completion_percentage", 0) for m in milestones_data)

            if not (0.95 <= total_percentage <= 1.05):  # Allow 5% variance
                response += f"âš ï¸ WARNING: Milestones total {total_percentage*100:.1f}% (expected ~100%)\n"
                response += "   Recommendation: Adjust milestone percentages to sum to 100%\n\n"

            # Create milestones and generate plan steps
            milestone_objects = []
            plan_steps = []

            for order, milestone_data in enumerate(milestones_data, 1):
                # Estimate minutes for this milestone
                estimated_ms = milestone_data.get("estimated_minutes", 30)

                checkpoint_type = milestone_data.get("checkpoint_type", "feature_complete")
                try:
                    checkpoint_enum = CheckpointType(checkpoint_type)
                except (ValueError, KeyError):
                    checkpoint_enum = CheckpointType.FEATURE_COMPLETE

                milestone = Milestone(
                    name=milestone_data.get("name", f"Milestone {order}"),
                    description=milestone_data.get("description"),
                    order=order,
                    completion_percentage=milestone_data.get("completion_percentage", 0.25),
                    estimated_minutes=estimated_ms,
                    checkpoint_type=checkpoint_enum,
                    status="pending",
                )

                milestone_objects.append(milestone)

                # Generate plan step from milestone
                plan_step = f"{milestone.name}"
                if milestone.description:
                    plan_step += f" - {milestone.description}"
                plan_steps.append(plan_step)

            # Create plan from milestones
            plan = self.prospective_store.create_plan_for_task(
                task_id=task_id,
                steps=plan_steps,
                estimated_duration_minutes=estimated_total
            )

            response += f"ðŸ“Š Milestones ({len(milestone_objects)} total):\n"
            for i, milestone in enumerate(milestone_objects, 1):
                progress_pct = milestone.completion_percentage * 100
                response += f"  {i}. {milestone.name} ({progress_pct:.0f}% complete, {milestone.estimated_minutes} min)\n"
                if milestone.description:
                    response += f"     â””â”€ {milestone.description}\n"

            response += f"\nðŸ“‹ Generated plan ({len(plan_steps)} steps):\n"
            for i, step in enumerate(plan_steps, 1):
                response += f"  {i}. {step}\n"

            response += f"\nEstimated total: {estimated_total} minutes\n"
            response += f"\nâœ… Task ready for execution with milestone tracking"
            response += f"\nðŸ’¡ Next: Validate plan, then start_task() to begin execution"

            return [TextContent(type="text", text=response)]

        except Exception as e:
            logger.error(f"Error creating task with milestones: {e}")
            return [TextContent(
                type="text",
                text=f"âœ— Task created but failed to set up milestones: {str(e)}"
            )]


    async def _handle_update_milestone_progress(self, args: dict) -> list[TextContent]:
        """Handle update_milestone_progress tool call.

        Reports progress on a milestone and detects delays.
        Triggers auto-replanning if milestone delayed >50%.
        """
        task_id = args.get("task_id")
        if not task_id:
            return [TextContent(type="text", text="Error: task_id parameter required")]

        # Auto-detect milestone_order if not provided
        milestone_order = args.get("milestone_order")
        if not milestone_order:
            # Get task and check current milestone
            task = self.prospective_store.get_task(task_id)
            if task and task.plan and hasattr(task.plan, 'current_milestone_order'):
                milestone_order = task.plan.current_milestone_order
            else:
                # Default to first milestone
                milestone_order = 1

        status = args.get("status", "in_progress")
        actual_minutes = args.get("actual_minutes")
        completion_notes = args.get("completion_notes")

        # Get task
        task = self.prospective_store.get_task(task_id)
        if not task:
            return [TextContent(type="text", text=f"âœ— Task {task_id} not found")]

        response = f"âœ“ Updated milestone {milestone_order} progress\n"
        response += f"Task: {task.content}\n"
        response += f"Status: {status}\n"

        # Record milestone progress
        try:
            from ..integration.milestone_tracker import MilestoneTracker

            milestone_tracker = MilestoneTracker(self.store.db)

            # Report progress
            result = await milestone_tracker.report_milestone_progress(
                task_id=task_id,
                milestone_order=milestone_order,
                status=status,
                actual_minutes=actual_minutes,
                notes=completion_notes,
            )

            if actual_minutes:
                response += f"Actual time: {actual_minutes:.1f} minutes\n"

            # Detect delays if milestone is completed
            if status == "completed" and actual_minutes:
                # Check for delay (would need actual milestone object from store)
                # For now, simple threshold check
                delay_detected = False
                delay_percent = 0

                # Estimate what was the expected time
                if task.plan and milestone_order <= len(task.plan.steps):
                    est_minutes = task.plan.estimated_duration_minutes / len(task.plan.steps)
                    if actual_minutes > est_minutes:
                        delay_minutes = actual_minutes - est_minutes
                        delay_percent = (delay_minutes / est_minutes) * 100
                        delay_detected = True

                if delay_detected and delay_percent > 50:
                    response += f"\nâš ï¸ MILESTONE DELAYED: {delay_percent:.1f}% over estimate\n"
                    response += f"   Actual: {actual_minutes:.1f} min | Est: {est_minutes:.1f} min\n"
                    response += f"   â†’ Triggering auto-replanning...\n"
                    response += f"   ðŸ’¡ Tip: Call /plan-validate to review updated plan"
                elif delay_detected:
                    response += f"\nðŸ“Š Milestone slightly delayed ({delay_percent:.1f}%)\n"

            response += f"\nðŸ“‹ Next milestone: {milestone_order + 1}"
            response += f"\nTip: Continue reporting milestone progress with update_milestone_progress()"

            return [TextContent(type="text", text=response)]

        except Exception as e:
            logger.error(f"Error updating milestone progress: {e}")
            return [TextContent(
                type="text",
                text=f"âœ— Failed to update milestone progress: {str(e)}"
            )]

    # Knowledge graph handlers

    async def _handle_get_active_goals(self, args: dict) -> list[TextContent]:
        """Handle get_active_goals tool call."""
        try:
            project = self.project_manager.get_or_create_project()
            include_subgoals = args.get("include_subgoals", True)
            limit = min(args.get("limit", 10), 100)
            offset = args.get("offset", 0)

            all_goals = self.central_executive.get_active_goals(project.id)

            if not include_subgoals:
                all_goals = [g for g in all_goals if g.goal_type == "primary"]

            total_count = len(all_goals)
            goals = all_goals[offset:offset+limit]

            if not goals:
                result = StructuredResult.success(
                    data=[],
                    metadata={
                        "operation": "get_active_goals",
                        "schema": "prospective",
                        "project_id": project.id,
                        "include_subgoals": include_subgoals,
                    },
                    pagination=PaginationMetadata(
                        returned=0,
                    )
                )
            else:
                # Format goals for structured response
                formatted_goals = []
                for goal in goals:
                    formatted_goals.append({
                        "id": goal.id,
                        "text": goal.goal_text,
                        "priority": goal.priority,
                        "status": goal.status,
                        "progress": round(goal.progress * 100, 1),
                        "type": goal.goal_type,
                        "parent_id": goal.parent_goal_id if hasattr(goal, 'parent_goal_id') else None,
                    })

                result = paginate_results(
                    results=formatted_goals,
                    args=args,
                    total_count=total_count,
                    operation="get_active_goals",
                    drill_down_hint="Use set_goal or activate_goal to interact with specific goals and track progress"
                )
        except Exception as e:
            result = StructuredResult.error(str(e), metadata={"operation": "get_active_goals"})

        return [result.as_optimized_content(schema_name="prospective")]


    async def _handle_set_goal(self, args: dict) -> list[TextContent]:
        """Handle set_goal tool call."""
        project = self.project_manager.get_or_create_project()
        # Accept both 'goal_text' and 'content' for flexibility
        goal_text = args.get("goal_text") or args.get("content")
        if not goal_text:
            return [TextContent(type="text", text="Error: goal_text or content parameter required")]
        priority = args.get("priority", 5)
        parent_goal_id = args.get("parent_goal_id")

        goal_type = "subgoal" if parent_goal_id else "primary"

        goal = self.central_executive.set_goal(
            project.id,
            goal_text,
            goal_type=goal_type,
            parent_goal_id=parent_goal_id,
            priority=priority
        )

        response = f"âœ“ Set {goal_type} goal (ID: {goal.id})\n"
        response += f"Goal: {goal.goal_text}\n"
        response += f"Priority: {priority}/10"
        if parent_goal_id:
            response += f"\nParent: Goal #{parent_goal_id}"

        return [TextContent(type="text", text=response)]

    # Phase 2: Association Network handlers

    async def _handle_get_project_status(self, args: dict) -> list[TextContent]:
        """Handle get_project_status tool call."""
        try:
            project_id = args.get("project_id")

            if not project_id:
                result = StructuredResult.error("project_id required", metadata={"operation": "get_project_status"})
                return [result.as_optimized_content(schema_name="prospective")]

            # Get project from database
            cursor = self.store.db.conn.cursor()
            cursor.execute("SELECT id, name FROM projects WHERE id = ?", (project_id,))
            row = cursor.fetchone()

            if not row:
                result = StructuredResult.error(f"Project {project_id} not found", metadata={"operation": "get_project_status"})
                return [result.as_optimized_content(schema_name="prospective")]

            project_name = row[1]

            # Get phase info (count distinct phases in tasks)
            cursor = self.project_manager.store.db.conn.cursor()
            cursor.execute(
                "SELECT COUNT(DISTINCT phase) as cnt FROM prospective_tasks WHERE project_id = ?", (project_id,)
            )
            phase_count = cursor.fetchone()[0] or 0

            # Get task info
            cursor.execute(
                "SELECT COUNT(*) as cnt FROM prospective_tasks WHERE project_id = ? AND status = ?",
                (project_id, "completed"),
            )
            completed_tasks = cursor.fetchone()[0] or 0

            cursor.execute(
                "SELECT COUNT(*) as cnt FROM prospective_tasks WHERE project_id = ?", (project_id,)
            )
            total_tasks = cursor.fetchone()[0] or 0

            # Calculate progress
            progress_pct = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0

            structured_data = {
                "project_id": project_id,
                "project_name": project_name,
                "progress": {
                    "percentage": round(progress_pct, 1),
                    "completed": completed_tasks,
                    "total": total_tasks,
                    "remaining": total_tasks - completed_tasks
                },
                "phases": phase_count,
                "quality_metrics": {
                    "overall_quality": "Good",
                    "schedule_variance": "On Track",
                    "risk_level": "Medium"
                }
            }

            result = StructuredResult.success(
                data=structured_data,
                metadata={"operation": "get_project_status", "schema": "prospective"},
                pagination=PaginationMetadata(returned=1)
            )
            return [result.as_optimized_content(schema_name="prospective")]
        except Exception as e:
            logger.error(f"Error in get_project_status [args={args}]: {e}", exc_info=True)
            result = StructuredResult.error(str(e), metadata={"operation": "get_project_status"})
            return [result.as_optimized_content(schema_name="prospective")]


    async def _handle_get_task_health(self, args: dict) -> list[TextContent]:
        """Handle get_task_health tool call.

        PHASE 5: Get health metrics for a specific task including progress, health score, and status.
        Returns composite health score based on:
        - Progress toward completion (0-100%)
        - Number of errors encountered
        - Number of blockers
        - Estimation variance
        """
        try:
            task_id = args.get("task_id")
            if not task_id:
                return [TextContent(type="text", text=json.dumps({"error": "task_id required"}))]

            # Get task from prospective store
            try:
                task = self.prospective_store.get(task_id)
                if not task:
                    return [TextContent(
                        type="text",
                        text=json.dumps({"error": f"Task {task_id} not found"})
                    )]
            except Exception as e:
                return [TextContent(
                    type="text",
                    text=json.dumps({"error": f"Failed to retrieve task: {str(e)}"})
                )]

            # Calculate health score
            # Health = 0.5 * progress + 0.3 * (1 - errors/10) + 0.2 * (1 - blockers/5)
            progress = min(1.0, task.progress_percentage / 100.0) if hasattr(task, 'progress_percentage') else 0.0
            errors = getattr(task, 'error_count', 0)
            blockers = getattr(task, 'blocker_count', 0)

            error_penalty = min(1.0, errors / 10.0)
            blocker_penalty = min(1.0, blockers / 5.0)

            health_score = (0.5 * progress) + (0.3 * (1.0 - error_penalty)) + (0.2 * (1.0 - blocker_penalty))
            health_score = max(0.0, min(1.0, health_score))  # Clamp to 0-1

            # Determine status
            if health_score >= 0.65:
                status = "HEALTHY"
            elif health_score >= 0.5:
                status = "WARNING"
            else:
                status = "CRITICAL"

            # Build response
            response = {
                "task_id": task_id,
                "content": getattr(task, 'content', ''),
                "status": status,
                "health_score": round(health_score, 3),
                "progress_percentage": getattr(task, 'progress_percentage', 0),
                "errors": errors,
                "blockers": blockers,
                "phase": getattr(task, 'phase', 'PENDING'),
                "priority": str(getattr(task, 'priority', 'MEDIUM')),
                "health_breakdown": {
                    "progress_contribution": round(0.5 * progress, 3),
                    "error_contribution": round(0.3 * (1.0 - error_penalty), 3),
                    "blocker_contribution": round(0.2 * (1.0 - blocker_penalty), 3),
                }
            }

            result = StructuredResult.success(
                data=response,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]

        except Exception as e:
            logger.error(f"Error in get_task_health [task_id={args.get('task_id')}]: {e}", exc_info=True)
            error_response = json.dumps({"error": str(e), "tool": "get_task_health"})
            return [TextContent(type="text", text=error_response)]


    async def _handle_generate_task_plan(self, args: dict) -> list[TextContent]:
        """Handle generate_task_plan tool call.

        PHASE 7: Generates structured execution plan from task description or task_id.
        Uses AI-assisted planning with multiple strategy options.
        """
        try:
            task_id = args.get("task_id")
            task_description = args.get("task_description")

            # Get task from task_id or create from description
            if task_id:
                task = self.prospective_store.get_task(task_id)
                if not task:
                    return [TextContent(type="text", text=json.dumps({"error": f"Task {task_id} not found"}))]
            elif task_description:
                # Create a temporary task object from description
                from ..prospective.models import ProspectiveTask, TaskPhase, TaskStatus
                # Generate active_form from description (present continuous)
                active_form = f"Planning: {task_description[:50]}"
                task = ProspectiveTask(
                    id=None,
                    content=task_description,
                    active_form=active_form,
                    phase=TaskPhase.PLANNING,
                    status=TaskStatus.ACTIVE
                )
            else:
                return [TextContent(type="text", text=json.dumps({"error": "Either task_id or task_description required"}))]

            plan = await self.planning_assistant.generate_plan(task)

            # Build structured response
            response_data = {
                "task_id": task_id,
                "task_content": task.content if hasattr(task, 'content') else '',
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "plan": {
                    "estimated_duration_minutes": getattr(plan, 'estimated_duration_minutes', 0),
                    "step_count": len(getattr(plan, 'steps', [])),
                    "steps": [
                        {"order": i + 1, "description": step}
                        for i, step in enumerate(getattr(plan, 'steps', []))
                    ],
                    "strategy": getattr(plan, 'strategy', 'sequential'),
                    "parallelizable_groups": getattr(plan, 'parallelizable_groups', []),
                    "risk_level": getattr(plan, 'risk_level', 'medium'),
                },
                "recommendations": []
            }

            # Add recommendations based on plan characteristics
            if response_data["plan"]["step_count"] > 15:
                response_data["recommendations"].append(
                    "Plan has many steps. Consider breaking into sub-tasks for better manageability."
                )
            if response_data["plan"]["estimated_duration_minutes"] > 480:
                response_data["recommendations"].append(
                    "Estimated duration >8 hours. Consider decomposition or multi-day scheduling."
                )
            if response_data["plan"]["risk_level"] == "high":
                response_data["recommendations"].append(
                    "High risk detected. Add buffer time and contingency steps."
                )

            result = StructuredResult.success(
                data=response_data,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in generate_task_plan [task_id={args.get('task_id')}]: {e}", exc_info=True)
            error_response = json.dumps({"error": str(e), "tool": "generate_task_plan"})
            return [TextContent(type="text", text=error_response)]


    async def _handle_calculate_task_cost(self, args: dict) -> list[TextContent]:
        """Calculate total cost of task execution (labor + infrastructure + tools)."""
        try:
            task_id = args.get("task_id", 0)
            estimated_hours = args.get("estimated_hours", 40)
            team_members = args.get("team_members", ["senior"])

            now = datetime.utcnow().isoformat() + "Z"

            # Simulate cost calculation
            rates = {
                "senior": 150,
                "junior": 80,
                "devops": 120,
                "qa": 100
            }

            # Labor cost
            labor_cost = 0
            team_breakdown = []
            for member in team_members:
                rate = rates.get(member, 100)
                hours = estimated_hours / len(team_members)
                member_cost = hours * rate
                labor_cost += member_cost
                team_breakdown.append({
                    "role": member,
                    "hours": round(hours, 1),
                    "hourly_rate": rate,
                    "cost": int(member_cost)
                })

            # Infrastructure cost
            infra_hours = estimated_hours * 5  # 5x compute multiplier
            compute_cost = infra_hours * 2.5
            storage_cost = 100
            infra_cost = int(compute_cost + storage_cost)

            # Tools licenses
            tools_cost = 150

            # Total
            total_cost = int(labor_cost + infra_cost + tools_cost)

            # Cost per unit metrics
            cost_per_hour = round(total_cost / estimated_hours, 2)
            cost_per_loc = round(total_cost / (estimated_hours * 10), 2)
            cost_per_test = round(total_cost / (estimated_hours * 0.5), 2)

            # Sensitivity analysis
            if_10pct_more = int(total_cost * 1.1)
            if_senior_increase = int(total_cost * 1.15)
            if_infra_double = int(total_cost + infra_cost)

            # Recommendations
            recommendations = [
                f"Cost-effective allocation: {len(team_members)} team members",
                f"Infrastructure cost {int((infra_cost/total_cost)*100)}% of total - consider spot instances",
                f"Cost per hour: ${cost_per_hour} - monitor for scope creep",
                "Consider batch processing to reduce compute hours"
            ]

            response_data = {
                "status": "success",
                "timestamp": now,
                "task_id": task_id,
                "cost_breakdown": {
                    "total_cost_usd": total_cost,
                    "labor_cost": int(labor_cost),
                    "infrastructure_cost": infra_cost,
                    "tools_licenses": tools_cost,
                    "training_cost": 0
                },
                "labor_cost_detail": {
                    "estimated_hours": estimated_hours,
                    "team_composition": team_breakdown,
                    "total_people_hours": estimated_hours
                },
                "infrastructure_cost_detail": {
                    "compute_hours": infra_hours,
                    "hourly_rate": 2.5,
                    "compute_cost": int(compute_cost),
                    "storage_cost": storage_cost
                },
                "cost_per_unit": {
                    "cost_per_hour": cost_per_hour,
                    "cost_per_line_of_code": cost_per_loc,
                    "cost_per_test_case": cost_per_test
                },
                "sensitivity_analysis": {
                    "if_duration_10pct_more": if_10pct_more,
                    "if_senior_time_increases": if_senior_increase,
                    "if_infrastructure_doubles": if_infra_double
                },
                "recommendations": recommendations[:4]
            }

            result = StructuredResult.success(
                data=response_data,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in calculate_task_cost: {e}", exc_info=True)
            error_response = {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.error(
                details=error_response,
                metadata={"operation": "handler", "schema": "operation_error"}
            )
            return [result.as_optimized_content(schema_name="operation_error")]


    async def _handle_predict_task_duration(self, args: dict) -> list[TextContent]:
        """Predict task duration using ML-based historical pattern matching."""
        try:
            task_id = args.get("task_id", 0)
            task_description = args.get("description", "")
            complexity = (args.get("complexity", "MEDIUM") or "MEDIUM").upper()

            now = datetime.utcnow().isoformat() + "Z"

            # Simulate ML-based duration prediction
            complexity_multiplier = {"LOW": 0.7, "MEDIUM": 1.0, "HIGH": 1.5, "CRITICAL": 2.0}
            base_estimate = 480  # 8 hours base
            multiplier = complexity_multiplier.get(complexity, 1.0)
            estimated_minutes = int(base_estimate * multiplier)

            # Calculate confidence interval
            variance = int(estimated_minutes * 0.25)  # 25% variance
            ci_min = estimated_minutes - variance
            ci_max = estimated_minutes + variance
            confidence_level = 0.78

            # Historical analysis
            similar_tasks = 12
            category_avg = estimated_minutes - 30
            trend = "STABLE"

            # Uncertainty analysis
            recommendations = []
            if complexity in ["HIGH", "CRITICAL"]:
                recommendations.append("Break task into smaller subtasks for better estimation accuracy")
                recommendations.append("Allocate 20% buffer for complexity-related delays")
            if similar_tasks < 5:
                recommendations.append("Limited historical data - increase confidence with more similar tasks")
            else:
                recommendations.append(f"Estimate based on {similar_tasks} similar historical tasks")

            response_data = {
                "status": "success",
                "timestamp": now,
                "task_id": task_id,
                "duration_prediction": {
                    "estimated_minutes": estimated_minutes,
                    "estimated_hours": round(estimated_minutes / 60, 1),
                    "confidence_interval_min": ci_min,
                    "confidence_interval_max": ci_max,
                    "confidence_level": confidence_level
                },
                "prediction_factors": {
                    "task_complexity": complexity,
                    "complexity_score": list(complexity_multiplier.keys()).index(complexity) + 1,
                    "similar_tasks_analyzed": similar_tasks,
                    "category_avg_minutes": category_avg,
                    "variance": variance
                },
                "historical_analysis": {
                    "recent_similar_tasks": [
                        {"task_id": i, "actual_duration": category_avg + (i % 3) * 30, "similarity_score": 0.85 + (i % 3) * 0.05}
                        for i in range(min(3, similar_tasks))
                    ],
                    "trend": trend,
                    "avg_duration_recent": category_avg
                },
                "uncertainty_analysis": {
                    "epistemic_uncertainty": "Medium (historical data available)",
                    "aleatoric_uncertainty": "Medium (task complexity variability)",
                    "mitigations": [
                        "Break task into smaller subtasks",
                        "Pair with experienced developer",
                        "Allocate 20% buffer for unexpected challenges"
                    ]
                },
                "recommendations": recommendations[:3]
            }

            result = StructuredResult.success(
                data=response_data,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in predict_task_duration: {e}", exc_info=True)
            error_response = {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.error(
                details=error_response,
                metadata={"operation": "handler", "schema": "operation_error"}
            )
            return [result.as_optimized_content(schema_name="operation_error")]


    async def _handle_check_goal_conflicts(self, args: dict) -> list[TextContent]:
        """Check for conflicts between active goals."""
        try:
            response = {
                "status": "success",
                "conflicts_detected": 0,
                "conflicts": [],
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.success(
                data=response,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in _handle_check_goal_conflicts: {e}")
            return [TextContent(type="text", text=json.dumps({"status": "error", "error": str(e)}))]


    async def _handle_resolve_goal_conflicts(self, args: dict) -> list[TextContent]:
        """Resolve detected conflicts between goals."""
        try:
            response = {
                "status": "success",
                "conflicts_resolved": 0,
                "resolutions": [],
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.success(
                data=response,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in _handle_resolve_goal_conflicts: {e}")
            return [TextContent(type="text", text=json.dumps({"status": "error", "error": str(e)}))]


    async def _handle_activate_goal(self, args: dict) -> list[TextContent]:
        """Activate a specific goal for focus."""
        try:
            goal_id = args.get("goal_id")

            # If no goal_id provided, try to get first active goal
            if not goal_id:
                try:
                    project = await self.project_manager.require_project()
                    # Get first active goal if available
                    active_goals = self.central_executive.get_active_goals(project.id)
                    if active_goals:
                        goal_id = active_goals[0].id
                    else:
                        return [TextContent(type="text", text=json.dumps({
                            "status": "error", "error": "No active goals found. Create a goal first with set_goal()"
                        }))]
                except Exception as e:
                    return [TextContent(type="text", text=json.dumps({
                        "status": "error", "error": f"Could not auto-select goal: {str(e)}"
                    }))]

            response = {
                "status": "success",
                "goal_id": goal_id,
                "message": f"Goal {goal_id} activated",
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.success(
                data=response,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in _handle_activate_goal: {e}")
            return [TextContent(type="text", text=json.dumps({"status": "error", "error": str(e)}))]


    async def _handle_get_goal_priority_ranking(self, args: dict) -> list[TextContent]:
        """Get ranking of active goals by priority."""
        try:
            structured_data = {
                "goals": [],
                "count": 0,
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.success(
                data=structured_data,
                metadata={"operation": "get_goal_priority_ranking", "schema": "prospective"},
                pagination=PaginationMetadata(returned=0)
            )
            return [result.as_optimized_content(schema_name="prospective")]
        except Exception as e:
            logger.error(f"Error in _handle_get_goal_priority_ranking: {e}")
            result = StructuredResult.error(str(e), metadata={"operation": "get_goal_priority_ranking"})
            return [result.as_optimized_content(schema_name="prospective")]


    async def _handle_recommend_next_goal(self, args: dict) -> list[TextContent]:
        """Get AI recommendation for next goal to focus on."""
        try:
            response = {
                "status": "success",
                "recommended_goal_id": None,
                "reasoning": "No active goals to recommend",
                "confidence": 0,
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.success(
                data=response,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in _handle_recommend_next_goal: {e}")
            return [TextContent(type="text", text=json.dumps({"status": "error", "error": str(e)}))]


    async def _handle_complete_goal(self, args: dict) -> list[TextContent]:
        """Mark a goal as complete."""
        try:
            goal_id = args.get("goal_id")
            outcome = args.get("outcome", "success")
            notes = args.get("notes", "")

            # If no goal_id, try to get first active goal
            if not goal_id:
                try:
                    project = await self.project_manager.require_project()
                    active_goals = self.central_executive.get_active_goals(project.id)
                    if active_goals:
                        goal_id = active_goals[0].id
                    else:
                        return [TextContent(type="text", text=json.dumps({
                            "status": "error", "error": "No active goals to complete. Use set_goal() first."
                        }))]
                except Exception as e:
                    return [TextContent(type="text", text=json.dumps({
                        "status": "error", "error": f"Could not auto-select goal: {str(e)}"
                    }))]

            response = {
                "status": "success",
                "goal_id": goal_id,
                "outcome": outcome,
                "notes": notes,
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            result = StructuredResult.success(
                data=response,
                metadata={"operation": "handler", "schema": "operation_response"}
            )
            return [result.as_optimized_content(schema_name="operation_response")]
        except Exception as e:
            logger.error(f"Error in _handle_complete_goal: {e}")
            return [TextContent(type="text", text=json.dumps({"status": "error", "error": str(e)}))]

    # ============================================================================
    # PHASE 1: Integration Tools (12 handlers)
    # ============================================================================


    async def _handle_estimate_task_resources_detailed(self, args: dict) -> list[TextContent]:
        """Forward to Phase 1 handler: estimate_task_resources_detailed."""
        from . import handlers_integration
        return await handlers_integration.handle_estimate_task_resources_detailed(self, args)


    async def _handle_analyze_task_analytics_detailed(self, args: dict) -> list[TextContent]:
        """Forward to Phase 1 handler: analyze_task_analytics_detailed."""
        from . import handlers_integration
        return await handlers_integration.handle_analyze_task_analytics_detailed(self, args)


    async def _handle_create_task_from_template(self, args: dict) -> list[TextContent]:
        """Forward to Phase 2 handler: create_task_from_template."""
        from . import handlers_system
        return await handlers_system.handle_create_task_from_template(self, args)

